<!DOCTYPE html><html><head><meta http-equiv="Content-Security-Policy" content="default-src 'unsafe-eval' 'unsafe-inline' data:; script-src 'none';"></meta>
	<title>ZADZMO code</title>
	<link rel="stylesheet" href="data:text/css;base64,aDEgewoJYmFja2dyb3VuZDogIzVFQUJFRDsKCXBhZGRpbmc6IDVweDsKfQoKYTp2aXNpdGVkIHsKICAgICAgICBjb2xvcjogIzFDMUU1NzsKfQogCmNvZGUgewogICAgICAgIGZvbnQtZmFtaWx5OiBtb25vc3BhY2U7Cn0KCmxpIHsKICAgICAgICBwYWRkaW5nLWJvdHRvbTogM3B4Owp9CgpwcmUgewogICAgICAgIGJhY2tncm91bmQtY29sb3I6ICNEMUVERUI7CiAgICAgICAgd2lkdGg6IDgwJTsKICAgICAgICBib3JkZXI6IDFweCBkb3R0ZWQ7Cglmb250LXNpemU6IDE2cHg7Cn0KCmJvZHkgewoJcGFkZGluZy1sZWZ0OiAxNXB4OwoJd2lkdGg6IDgwJTsKCWZvbnQtZmFtaWx5OiBzYW5zLXNlcmlmOwoJZm9udC1zaXplOiAxNXB4OwoJYmFja2dyb3VuZC1jb2xvcjogI2ZmZmZmZjsgCgltYXJnaW46IDBweDsKfQoK">
	<link type="text/plain" rel="author" href="https://zadzmo.org/humans.txt">
<base href="https://zadzmo.org/code/nepenthes"></base></head>
<body>

<h1>Nepenthes</h1>
<p>This is a tarpit intended to catch web crawlers. Specifically, it's targetting crawlers that scrape data
for LLM's - but really, like the plants it is named after, it'll eat just about anything that finds it's
way inside.</p>
<p>It works by generating an endless sequences of pages, each of which with dozens of links, that simply go
back into a the tarpit. Pages are randomly generated, but in a deterministic way, causing them to appear
to be flat files that never change. Intentional delay is added to prevent crawlers from bogging down your
server, in addition to wasting their time. Lastly, optional Markov-babble can be added to the pages, to
give the crawlers something to scrape up and train their LLMs on, hopefully accelerating model collapse.</p>
<p><a href="https://zadzmo.org/nepenthes-demo">You can take a look at what this looks like, here. (Note: VERY slow page loads!)</a></p>
<h1>WARNING</h1>
<p>THIS IS DELIBERATELY MALICIOUS SOFTWARE INTENDED TO CAUSE HARMFUL ACTIVITY.
DO NOT DEPLOY IF YOU AREN'T FULLY COMFORTABLE WITH WHAT YOU ARE DOING.</p>
<h1>ANOTHER WARNING</h1>
<p>LLM scrapers are relentless and brutual. You may be able to keep them at bay
with this software - but it works by providing them with a neverending stream
of exactly what they are looking for. YOU ARE LIKELY TO EXPERIENCE SIGNIFICANT
CONTINUOUS CPU LOAD, ESPECIALLY WITH THE MARKOV MODULE ENABLED.</p>
<h1>YET ANOTHER WARNING</h1>
<p>There is not currently a way to differentiate between web crawlers that
are indexing sites for search purposes, vs crawlers that are training
AI models. ANY SITE THIS SOFTWARE IS APPLIED TO WILL LIKELY DISAPPEAR
FROM ALL SEARCH RESULTS.</p>
<h2>Latest Version</h2>
<p><a href="https://zadzmo.org/code/nepenthes/downloads/nepenthes-1.0.tar.gz">Nepenthes 1.0</a></p>
<p><a href="https://zadzmo.org/code/nepenthes/downloads/">All downloads</a></p>
<h2>Usage</h2>
<p>Expected usage is to hide the tarpit behind nginx or Apache, or whatever else you have implemented your
site in. Directly exposing it to the internet is ill advised. We want it to look as innocent and normal
as possible; in addition HTTP headers are used to configure the tarpit.</p>
<p>I'll be using nginx configurations for examples. Here's a real world snippet for the demo above:</p>
<pre><code>    location /nepenthes-demo/ {
            proxy_pass http://localhost:8893;
            proxy_set_header X-Prefix '/nepenthes-demo';
            proxy_set_header X-Forwarded-For $remote_addr;
            proxy_buffering off;
    }
</code></pre>
<p>You'll see several headers are added here: "X-Prefix" tells the tarpit that all links should go to that
path. Make this match what is in the 'location' directive. X-Forwarded-For is optional, but will make any
statistics gathered significantly more useful.</p>
<p>The proxy_buffering directive is important. LLM crawlers typically disconnect if not given a response within
a few seconds; Nepenthes counters this by drip-feeding a few bytes at a time. Buffering breaks this workaround.</p>
<p>You can have multiple proxies to an individual Nepenthes instance; simply set the X-Prefix header accordingly.</p>
<h2>Installation</h2>
<p>You can use Docker, or install manually.</p>
<p>A Dockerfile and compose.yaml is provided in the <a href="https://svn.zadzmo.org/repo/nepenthes/head/docker/">/docker directory.</a>
Simply tweak the configuration file to your preferences, 'docker compose up'. You will still need to bootstrap
a Markov corpus if you enable the feature (see next section.)</p>
<p>For Manual installation, you'll need to install Lua (5.4 preferred), SQLite (if using Markov), and OpenSSL.
The following Lua modules need to be installed - if they are all present in your package manager, use that;
otherwise you will need to install <a href="https://luarocks.org/">Luarocks</a> and use it to install the following:</p>
<ul>
<li><a href="https://luarocks.org/modules/daurnimator/cqueues">cqueues</a></li>
<li><a href="https://luarocks.org/modules/daurnimator/luaossl">ossl</a> (aka luaossl)</li>
<li><a href="https://luarocks.org/modules/gvvaughan/lpeg">lpeg</a></li>
<li><a href="https://luarocks.org/modules/hisham/lzlib">lzlib</a>
(or <a href="https://luarocks.org/modules/brimworks/lua-zlib">lua-zlib</a>, only one of the two needed)</li>
<li><a href="https://luarocks.org/modules/sparked435/luadbi-sqlite3">dbi-sqlite3</a> (aka luadbi-sqlite3)</li>
<li><a href="https://luarocks.org/modules/daurnimator/lunix">unix</a> (aka lunix)</li>
</ul>
<p>Create a nepenthes user (you REALLY don't want this running as root.) Let's assume the user's home
directory is also your install directory.</p>
<pre><code>useradd -m nepenthes
</code></pre>
<p>Unpack the tarball:</p>
<pre><code>cd scratch/
tar -xvzf nepenthes-1.0.tar.gz
    cp -r nepenthes-1.0/* /home/nepenthes/
</code></pre>
<p>Tweak config.yml as you prefer (see below for documentation.) Then you're
ready to start:</p>
<pre><code>    su -l -u nepenthes /home/nepenthes/nepenthes /home/nepenthes/config.yml
</code></pre>
<p>Sending SIGTERM or SIGINT will shut the process down.</p>
<h2>Bootstrapping the Markov Babbler</h2>
<p>The Markov feature requires a trained corpus to babble from. One was intentionally omitted because, ideally,
everyone's tarpits should look different to evade detection. Find a source of text in whatever language you
prefer; there's lots of research corpuses out there, or possibly pull in some very long Wikipedia articles,
maybe grab some books from Project Gutenberg, the Unix fortune file, it really doesn't matter at all. Be creative!</p>
<p>Training is accomplished by sending data to a POST endpoint. This only needs to be done once. Sending training
data more than once cumulatively adds to the existing corpus, allowing you to mix different texts - or train in
chunks.</p>
<p>Once you have your body of text, assuming it's called corpus.txt, in your working directory, and you're running
with the default port:</p>
<pre><code>curl -XPOST -d ./@corpus.txt -H'Content-type: text/plain' http://localhost:8893/train
</code></pre>
<p>This could take a very, VERY long time - possibly hours. curl may potentially time out. See
<a href="https://svn.zadzmo.org/repo/nepenthes/head/load.sh">load.sh</a> in the nepenthes distribution for a script that
incrementally loads training data.</p>
<p>The Markov module returns an empt string if there is no corpus. Thus, the tarpit will continue to function
as a tarpit without a corpus loaded. The extra CPU consumed for this check is almost nothing.</p>
<h2>Statistics</h2>
<p>Want to see what prey you've caught? There are several statistics endpoints, all returning JSON. To see everything:</p>
<pre><code>http://{http_host:http_port}/stats
</code></pre>
<p>To see user agent strings only:</p>
<pre><code>http://{http_host:http_port}/stats/agents
</code></pre>
<p>Or IP addresses only:
3
http://{http_host:http_port}/stats/ips/</p>
<p>These can get quite big; so it's possible to filter both 'agents' and 'ips', simply add a minimum hit count to the
URL. For example, to see a list of all IPs that have visted more than 100 times:</p>
<pre><code>http://{http_host:http_port}/stats/ips/100
</code></pre>
<p>Simply curl the URLs, pipe into 'jq' to pretty-print as desired. Script away!</p>
<h2>Nepenthes used Defensively</h2>
<p>A link to a Nepenthes location from your site will flood out valid URLs
within your site's domain name, making it unlikely the crawler will access
real content.</p>
<p>In addition, the aggregated statistics will provide a list of IP addresses
that are almost certainly crawlers and not real users. Use this list to
create ACLs that block those IPs from reaching your content - either return
403, 404, or just block at the firewall level.</p>
<p>Integration with fail2ban or blocklistd (or similar) is a future possibility,
allowing realtime reactions to crawlers, but not currently implemented.</p>
<p>Using Nepenthes defensively, it would be ideal to turn off the Markov
module, and set both max_delay and min_delay to something large, as a
way to conserve your CPU.</p>
<h2>Nepenthes used Offensively</h2>
<p>Let's say you've got horsepower and bandwidth to burn, and just want to
see these AI models burn. Nepenthes has what you need:</p>
<p>Don't make any attempt to block crawlers with the IP stats. Put the delay
times as low as you are comfortable with. Train a big Markov corpus and
leave the Markov module enabled, set the maximum babble size to something
big. In short, let them suck down as much bullshit as they have diskspace
for and choke on it.</p>
<h2>Configuration File</h2>
<p>All possible directives in config.yaml:</p>
<ul>
<li>http_host : sets the host that Nepenthes will listen on; default is localhost only.</li>
<li>http_port : sets the listening port number; default 8893</li>
<li>prefix: Prefix all generated links should be given. Can be overriden with the X-Prefix HTTP header. Defaults to nothing.</li>
<li>templates: Path to the template files. This should be the '/templates' directory inside your Nepenthes installation.</li>
<li>detach: If true, Nepenthes will fork into the background and redirect logging output to Syslog.</li>
<li>pidfile: Path to drop a pid file after daemonization. If empty, no pid file is created.</li>
<li>max_wait: Longest amount of delay to add to every request. Increase to slow down crawlers; too slow they might not come back.</li>
<li>min_wait: The smallest amount of delay to add to every request. A random value is chosen between max_wait and min_wait.</li>
<li>real_ip_header: Changes the name of the X-Forwarded-For header that communicates the actual client IP address for statistics gathering.</li>
<li>prefix_header: Changes the name of the X-Prefix header that overrides the prefix configuration variable.</li>
<li>forget_time: length of time, in seconds, that a given user-agent can go missing before being deleted from the statistics table.</li>
<li>forget_hits: A user-agent that generates more than this number of requests will not be deleted from the statistics table.</li>
<li>persist_stats: A path to write a JSON file to, that allows statistics to survive across crashes/restarts, etc</li>
<li>seed_file: Specifies location of persistent unique instance identifier. This allows two instances with the same corpus to have different looking tarpits.</li>
<li>words: path to a dictionary file, usually '/usr/share/dict/words', but could vary depending on your OS.</li>
<li>markov: Path to a SQLite database containing a Markov corpus. If not specified, the Markov feature is disabled.</li>
<li>markov_min: Minimum number of words to babble on a page.</li>
<li>markov_max: Maximum number of words to babble on a page. Very large values can cause serious CPU load.</li>
</ul>
<h2>History</h2>
<p>Version numbers use a simple process: If the only changes are fully backwards
compatible, the minor number changes. If the user/administrator needs to change
anything after or part of the upgrade, the major number changes and the minor
number resets to zero.</p>
<p>v1.0: Initial release</p>

<div class="foot">
<p><a href="https://zadzmo.org/notice.md">Site Information</a></p>

</div>


</body></html>
